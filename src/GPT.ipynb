{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5514c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90f0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [123, 0, 23, 5] -> [[..512..], [...512...], ...]\n",
    "        return self.embed(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec40f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant positional encoding matrix\n",
    "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
    "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
    "        pe_matrix = pe_matrix.unsqueeze(0)     # Add one dimension for batch size\n",
    "        self.register_buffer('pe', pe_matrix)  # Register as persistent buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a sentence after embedding with dim (batch, number of words, vector dimension)\n",
    "        seq_len = x.size()[1]\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d36cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given Query, Key, Value, calculate the final weighted value\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    # Shape of q and k are the same, both are (batch_size, seq_len, d_k)\n",
    "    # Shape of v is (batch_size, seq_len, d_v)\n",
    "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])  # size (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # Apply mask to scores\n",
    "    # \n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
    "        \n",
    "    # Softmax along the last dimension\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aacbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_v = d_model//n_heads\n",
    "        \n",
    "        # self attention linear layers\n",
    "        # Linear layers for q, k, v vectors generation in different heads\n",
    "        self.q_linear_layers = []\n",
    "        self.k_linear_layers = []\n",
    "        self.v_linear_layers = []\n",
    "        for i in range(n_heads):\n",
    "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        multi_head_attention_outputs = []\n",
    "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
    "                                                self.k_linear_layers,\n",
    "                                                self.v_linear_layers):\n",
    "            new_q = q_linear(q)  # size: (batch_size, seq_len, d_k)\n",
    "            new_k = k_linear(k)  # size: (batch_size, seq_len, d_k)\n",
    "            new_v = v_linear(v)  # size (batch_size, seq_len, d_v)\n",
    "            \n",
    "            # Scaled Dot-Product attention\n",
    "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)  # (batch_size, seq_len, d_v)\n",
    "            multi_head_attention_outputs.append(head_v)\n",
    "            \n",
    "        # Concat\n",
    "        #import pdb; pdb.set_trace()\n",
    "        concat = torch.cat(multi_head_attention_outputs, -1)  # (batch_size, seq_len, n_heads*d_v)\n",
    "        \n",
    "        # Linear layer to recover to original shap\n",
    "        output = self.out(concat)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330f2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6f1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len, d_model)\n",
    "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
    "        x_tilde = self.alpha*x_hat + self.beta\n",
    "        return x_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bfb01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_layer(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b85bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.norm_1(x)\n",
    "        x = x + self.multi_head_attention(x, x, x, mask)\n",
    "        x = self.norm_2(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "883964f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self, d_model, N, n_heads, vocab_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.gpt_layers = clone_layer(GPTBlock(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for block in self.gpt_layers:\n",
    "            x = block(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7661ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import itertools\n",
    "from torchtext import data\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.split('\\n')\n",
    "    exceptions = ['begin', 'end']\n",
    "    ignorelist = ['&', '$', '#', '_', '{', '}', '~', '^']\n",
    "    exceptions = ['\\\\\\\\'+tag+\"{[^{}]*}\" for tag in exceptions]\n",
    "    pattern = exceptions + [\"\\\\\\\\\\w*|{|}|\\$|\\[|\\]|<text>|\\s*\"]\n",
    "    pattern = '|'.join(pattern)\n",
    "    pattern = r\"(%s)\" % pattern\n",
    "    \n",
    "    tokenized = [list(filter(None, (lambda x: re.split(pattern, x))(line))) + ['\\n'] for line in text]\n",
    "    \n",
    "    return list(itertools.chain.from_iterable(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba26458d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e52d9d79634bdeadc88966b807f348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH = \"./../data/papers/\"\n",
    "\n",
    "tex_files = glob.glob(PATH + \"*.tex\", recursive = True)\n",
    "\n",
    "wordCount = 0\n",
    "sentCount = 0\n",
    "docCount = 0\n",
    "vocab = set()\n",
    "vocabDict = {} # Vocabulary dictionary\n",
    "vocabCount = 0\n",
    "vocabCumSum = [] # To plot Vocab vs Words\n",
    "wordCumSum = [] # To plot Vocab vs Words\n",
    "\n",
    "for file in tqdm(tex_files):\n",
    "    text = open(file, \"rt\", encoding = \"utf8\").read()\n",
    "    tokens = tokenizer(text)\n",
    "    docCount += 1\n",
    "    \n",
    "    for token in tokens:\n",
    "        wordCount += 1\n",
    "        token = token.lower()\n",
    "        if token not in vocab:\n",
    "            vocab.add(token)\n",
    "            vocabDict[token] = 1\n",
    "            vocabCount += 1\n",
    "        else:\n",
    "            vocabDict[token] += 1\n",
    "    \n",
    "    vocabCumSum += [vocabCount]\n",
    "    wordCumSum += [wordCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88db8994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339216"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabDF = pd.DataFrame(vocabDict.items())\n",
    "\n",
    "len(vocabDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f08f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Argodep\\AppData\\Local\\Temp\\ipykernel_8380\\1210355278.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = create_mask(torch.tensor(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5564, -2.1160, -2.1147,  ...,  1.0211, -2.6412, -1.4760],\n",
       "         [ 0.7566, -2.2017, -1.9696,  ...,  0.8697, -2.6994, -1.1817],\n",
       "         [ 0.7672, -1.9968, -2.1514,  ...,  0.9146, -2.2939, -1.0523],\n",
       "         [ 0.5217, -1.8834, -1.9739,  ...,  0.7363, -2.4286, -0.7821],\n",
       "         [ 0.6801, -2.0235, -1.6651,  ...,  0.5793, -2.2954, -1.1591]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "N = 12\n",
    "vocab_size = 100\n",
    "\n",
    "model = GPT(d_model, N, n_heads, vocab_size)\n",
    "\n",
    "def create_mask(text):\n",
    "    pad = 1 #SRC.vocab.stoi['<pad>']    \n",
    "    # Input mask\n",
    "    pad_mask = (text != pad).unsqueeze(1)\n",
    "    seq_len = text.size(1)\n",
    "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
    "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
    "    mask = pad_mask & nopeak_mask\n",
    "    print(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "x = torch.tensor([[0,0,0,0,0]])\n",
    "                 \n",
    "mask = create_mask(torch.tensor(x))\n",
    "model(x, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
